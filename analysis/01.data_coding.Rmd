---
title: "Data Coding"
author: "Erin M. Buchanan"
date: "Last Knitted `r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document outlines the creation of the final raw data for presentation of all percentages and reports. In this code, we clean up coding issues (e.g., Excel/Google Sheets transformation of article IDs into scientific notation), check for accidental duplicates of coding, and examine the results from our proportion of round two coding. At the end of the document, we have created a finalized dataset for presentation of raw results. 

## Libraries

```{r}
library(rio)
library(dplyr)
library(stringi)
library(tidyr)
library(psych)
library(ggplot2)
library(maps)
library(countrycode)
library(webr)
library(patchwork)
```

## Get the Raw Data

```{r}
DF <- import("../analysis/coded_responses.xlsx")

# column 35 is empty
DF <- DF %>% select(-`...35`)

# take out pilot testing 2021-05-21 04:12:04 and before
DF <- DF %>% filter(Timestamp > as.POSIXct("2021-05-22"))

# these names are descriptive but no fun to type
colnames(DF) <- c("TimeStamp", "TimeStarted", "DateStarted", 
                  "Coder", "ArticleID", "ArticleIssues", "ArticleLanguage", 
                  "JIF", "StudyType", "PreReg", "PreRegWhere", "PreRegAccess", 
                  "PreRegAspects", "RawData", "RawDataAvl", "RawDataWhere", 
                  "RawDataAccess", "RawDataDocument", "ProcessData", 
                  "ProcessDataWhere", "ProcessDataAccess", "ProcessDataDocument", 
                  "AnalysisScript", "AnalysisScriptWhere", "AnalysisScriptAccess",
                  "Materials", "MaterialsWhere", "MaterialsAccess", "COI", 
                  "Replication", "OA", "TimeEnded", "DateEnded", "CountryAuthor", 
                  "EmpiricalStudyData", "JIFYear")

# list of information from scopus
prescreenDF <- import("../Prescreening/prescreening_round1.csv")
prescreenDF2 <- import("../data/prescreening_list_2021-07-04.txt")

# deal with the duplicate ID issue 
# we didn't code this one but we did code the other one
# as per github page (pulled original number)
prescreenDF$ID[prescreenDF$Title == "Quantified epistemic logics for reasoning about knowledge in multi-agent systems"] <- "e754dd1b"

# import code on who was doing what to check all are done
coder1 <- import("../data/randomized_prescreen_df_2021_10_21.csv")
coder2 <- import("../data/randomized_prescreen_df_rnd2_2022_03_14.csv") 
```

## Clean Up Data

```{r}
# merge information about publication year, field, pre versus post 
DF2 <- DF %>% 
  # year, year label, DOI
  left_join(
  (prescreenDF %>% select(ID, Year, year_split, clickable_doi, first_code_include)), 
  by = c("ArticleID" = "ID")
) 
  
# fix the coding issues
A_ID_fix <- DF2 %>% filter(is.na(year_split)) %>% select(ArticleID) 
A_ID_fix <- as.vector(A_ID_fix$ArticleID)
A_ID_fix <- paste0("\\b", A_ID_fix, "\\b")

A_ID_fixed <- c("96498326", "9586644", "86897574", "35147373", 
                "34004670", "33442876", "53780209", "15561653", 
                "4.16E+06", "98056778", "74a007fe", "9923056", 
                "67395332", "5.84E+59", "98963718", "20880301",
                "84326216", "30630572", "8.86E+11", "15561653", 
                "239b8bc4", "98963718", "86897574")

length(A_ID_fix) == length(A_ID_fixed)

DF$ArticleID <- stri_replace_all_regex(str = DF$ArticleID,
                             pattern = A_ID_fix, 
                             replacement = A_ID_fixed,
                             vectorize_all = FALSE)

# fix some other fun quirks
# i checked these were not also in coder 2 
coder1$ID[coder1$ID == "8.86e+11"] <- "8.86E+11"
coder1$ID[coder1$ID == "5.84e+59"] <- "5.84E+59"
coder1$ID[coder1$ID == "4160000"] <- "4.16E+06"

# now really merge once IDs are fixed
DF2 <- DF %>% left_join(
  (prescreenDF %>% select(ID, Year, year_split, clickable_doi)), 
  by = c("ArticleID" = "ID")
) %>% 
  # coder assignments 
  full_join(
  (coder1 %>% select(ID, coder)), 
  by = c("ArticleID" = "ID")
) %>% 
  full_join(
  (coder2 %>% select(ID, round2_coder)),
  by = c("ArticleID" = "ID")
  ) %>% 
  # article journal, source title
  left_join(
    (prescreenDF2 %>% select(ID, Source.title)), 
     by = c("ArticleID" = "ID")
  )

# look for missing articles 
DF2 %>% filter(is.na(TimeStamp)) %>% select(coder, ArticleID)

# look at duplicates by the same person 
dup_IDs <- DF2 %>% filter(duplicated(DF2 %>% select(ArticleID, Coder))) %>% pull(ArticleID)

DF2 %>% filter(ArticleID %in% dup_IDs)

# now, what I did was go to the git issue
# https://github.com/troettge/Transparency-Ling/issues/24
# looked at the articles each person did
# figured out which two were copy paste article errors for TR
# LK appears to be a real duplicate 
# TR cc497581 also appears to be a real duplicate 

# 0e3e42b8 comes after 64cf897b
DF$ArticleID[DF$ArticleID == "64cf897b"][2] <- "0e3e42b8"

# remove real duplicates 
DF <- DF %>% 
  filter(!duplicated(DF %>% select(ArticleID, Coder)))

# final data 
DF <- DF %>% left_join(
  (prescreenDF %>% select(ID, Year, year_split, clickable_doi)), 
  by = c("ArticleID" = "ID")
) %>% 
  # coder assignments 
  full_join(
  (coder1 %>% select(ID, coder)), 
  by = c("ArticleID" = "ID")
) %>% 
  full_join(
  (coder2 %>% select(ID, round2_coder)),
  by = c("ArticleID" = "ID")
  ) %>% 
  # article journal, source title
  left_join(
    (prescreenDF2 %>% select(ID, Source.title)), 
     by = c("ArticleID" = "ID")
  )
```

## Round 2 Coding

Figure out what to check and export that file for the second coding check. 

```{r}
# find articles that were coded for round two for a double check 
DF_Second <- DF %>% 
      filter(!is.na(round2_coder)) %>% 
      select(-c(TimeStamp, TimeStarted, DateStarted, OA,
                TimeEnded, DateEnded, Year, year_split, clickable_doi, 
                coder, Source.title, round2_coder)) %>% 
      group_by(ArticleID) %>% 
      mutate(coder_num = paste0("coder_", row_number())) %>% 
      ungroup() %>% 
      pivot_longer(cols = -c(ArticleID, coder_num, Coder)) %>% 
      pivot_wider(id_cols = c(ArticleID, name),
                  names_from = coder_num, 
                  values_from = c(value, Coder))
  
  # original code that excluded NA values and matches #
  # this excluded things I didn't mean to that #
  # were potentially not matches #
  # filter(!is.na(easy_screen)) %>% 
  # filter(easy_screen == F) %>% 
  # mutate(check = rep(c("EB", "KC", "IC", "CH"), length.out = nrow(.)))

export(DF_Second, "../analysis/second_coder_check.csv", row.names = F)
```

Import and examine the results from the second coding check. 

First, EMB noticed that she did not print out the article mismatches for review that had one NA value and one real value, which would generally be a mismatch. Given the group discussion described below and a few apparent miscodings, EMB recoded all the mismatches as described: (note that the original coding is available in the excel document imported below `second_coder_done.xlsx` on a second sheet)

- The "unclear" category was added to indicate it was unclear if it was a mismatch because the person may have skipped answering these questions if they did not think the article was about language or did not have access. Therefore, all articles with one coder NA value and one coder real text was coded as Unclear.
- A not available and not applicable was coded as a match.
- A partial category was added for answers that overlapped but weren't exactly the same.

We also separately coded JIF because only a few members had access to the information in a specialized database. 

```{r}
# import the second coder information EMB coded
DF_Second_import <- import("../analysis/second_coder_done.xlsx")

# create a dataset of second matches to examine 
DF_Second_match <- DF %>% filter(!is.na(round2_coder)) %>% 
  select(-c(TimeStamp, TimeStarted, DateStarted, OA, 
            TimeEnded, DateEnded, Year, year_split, clickable_doi, 
            coder, Source.title, round2_coder)) %>% 
  group_by(ArticleID) %>% 
  mutate(coder_num = paste0("coder_", row_number())) %>% 
  ungroup() %>% 
  select(-Coder) %>% 
  pivot_longer(cols = -c(ArticleID, coder_num)) %>% 
  pivot_wider(id_cols = c(ArticleID, name), 
              names_from = coder_num, 
              values_from = value) %>% 
  mutate(easy_screen = coder_1 == coder_2) %>% 
  left_join((DF_Second_import %>% select(ArticleID, answer, name, corrected_answer)), 
            by = c("ArticleID" = "ArticleID", 
                   "name" = "name"))

# comparison table of screenings (easy is exact match)
table("easy" = DF_Second_match$easy_screen, 
      "coded" = DF_Second_match$answer, 
      useNA = "ifany")

# overall agreement
table(DF_Second_match$answer, useNA = "ifany")

# table of agreement before exclusion of first question mismatches
table(DF_Second_match$answer, 
      DF_Second_match$name,
      useNA = "ifany") / 120 * 100
```

Overall these results indicated a need to examine:

- Articles Issues (this was true before recoding)
- JIF was recoded (see below)

In a group discussion, we first discovered that the "mismatches" were often from one person indicating they did not have access to the article or disagreeing over if the article was about language. This first question would then lead to all other questions being in disagreement (and likely the unclear category mentioned above - therefore this was a problem in the `ArticleIssues` column). 

Since the goal was to reach 250 articles in each pre and post era, we excluded all articles that were a mismatch because of these two issues (did not have access, article about language). We then recalculated the agreement statistics for the double coded articles in which both articles were actually coded. 

```{r}
# find articles that match 
agree_list <- DF_Second_match %>% 
  filter(answer == TRUE) %>% 
  filter(name == "ArticleIssues") %>% 
  pull(ArticleID)

length(agree_list)

# create a DF of just ones that agree past Article Issues
DF_Second_match_agree <- DF_Second_match %>% filter(ArticleID %in% agree_list)

# now examine for mismatches 
table(DF_Second_match_agree$answer, 
      DF_Second_match_agree$name,
      useNA = "ifany") / 98 * 100
```

After excluding articles that did not agree on the first item (and recoding above), we found that coders appeared to partially to completely agree at our threshold level. 

## Number of Articles

```{r}
# make DF that's not the second coding
# break in data here 2022-03-10 17:13:23
# but don't forget that TR did one later because we realized it was missing 
articleRow <- DF %>% filter(ArticleID == "e86871b8")
DF <- DF %>% 
  filter(TimeStamp < as.POSIXct("2022-03-14")) %>% 
  bind_rows(articleRow)

# check rows
nrow(DF)

# check IDs
length(table(DF$ArticleID))

# did we reach our goal?
table(DF$ArticleIssues, DF$year_split, useNA = "ifany")

# table of double match
# grab all articles that weren't double coded
not_double <- DF %>% filter(!(ArticleID %in% DF_Second_match$ArticleID)) %>% pull(ArticleID)

# only articles that we did not double code and ones we did 
# double code and agreed should be included
DF$Agree <- DF$ArticleID %in% c(not_double, agree_list)

# did we meet our goal of at least 250? 
table(DF$ArticleIssues[DF$Agree], 
      DF$year_split[DF$Agree], useNA = "ifany")
```

## Merge Partial/Unclear Answers

```{r}
# don't judge me on the loop ok 
# import answers we should fix 
DF_fix <- import("../analysis/coder_abritration.xlsx") %>% 
  filter(include == "included") %>% 
  filter(!is.na(corrected_answer))

# loop over and fix those answers - basically merging answers as 
# we found that things mostly needed to be merged together 
for (i in 1:nrow(DF_fix)){
  DF[
    DF$ArticleID == DF_fix$ArticleID[i], # article match 
    DF_fix$name[i] # variable match
    ] <- DF_fix$corrected_answer[i]
}
```

## Export Data

```{r}
export(DF, "01.data_coding.csv", row.names = FALSE)
```

